---
title: "Assignment 2 v.00"
author: "Fabrizio Fiorini"
date: "2/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Module 4 - k-NN Classification Model

This assignment consists on applying a k-NN model on a dataset of 5,000 customers of the Universal Bank. The object is to predict whether a new customer would accept a personal loan offer.

The 14 variables that form the original dataset are the following:

```{r}
#install (if necessary) and load the required libraries
library(caret)
library(FNN)
library(ggplot2)
library("gmodels")
library(ISLR)
library(dplyr)
library(fastDummies)
```


## Import and Exploration of the dataframe

We use the read.csv function to load the file that contains the dataframe of Universal Bank's customers. Then, we will run some exploratory analisys to get familiar with the dataset.

```{r}
#Importing the database
customers <- read.csv("UniversalBank.csv")
```

```{r}
#Exploring the data by previewing the first 6 rows and all the 14 variables.
head(customers)

#Looking at the structure of the dataset.
str(customers)

#Descriptive statistics
summary(customers)

#Verifing that Education variable is a categorical variable with more than 2 possible values.
unique(customers$Education)
```

## Transformation and Subsetting

Since the Education variable contains 3 unique values (1, 2 or 3), we have to transform it in dummy variable before normalizing data.

```{r}
#Selecting Education and creating dummy variables
dummy_df <- dummy_cols(customers, select_columns = c("Education"))
head(dummy_df)
```

Now, the dataset consists of 5,000 rows and 17 variables.
Since we do not need all the variables in the original dataset, we can subset it by removing variables such as ID, ZIP code and the original Education variable. In fact, we now have 3 distinct dummy variables for Education, that are at the right-end of the dataset.

```{r}
#Subsetting the data to remove unnecessary variables (ID, ZIP code, )
subset <- select(dummy_df, 2:4, 6:7, 9:17)
head(subset)
```

The subset consists of 5,000 rows and 14 variables.



## Partitioning of the data and Plotting

In order to teach the model to predict classification of new data, we need to split data in 2 sets, Training set (60%) and Validation set (40%).

```{r}
set.seed(123)

sample = createDataPartition(subset$Income, p=0.60, list=FALSE)

train_df = subset[sample, ]
valid_df = subset[-sample, ]
```


summary(train_df$Income)

summary(valid_df$Income)

```{r}
ggplot(train_df, aes(x=Income, y=Age, colour=train_df$Personal.Loan)) + geom_point()
```

This chart shows us that the majority of customers have an income below 100,000 while they are evenly spread between 25 and 65 years old. Moreover, a customer earning more than 100,000 is a lot more     to apply for a personal loan.



## Normalization process and k-NN Model

# Normalizing data

A k-NN model is very sensible on the different scales of the variables, so in order to reduce the bias of certain variables on the prediction, we need to normalize the data.
For this process, we use preProcess and predict functions from the Caret package.
From the initial exploration phase, we know that from the 8th to the 14th column, we only have categorical variables (Education, Securities Account, etc.) that contains only 0 or 1 values. The variable to be predicted (Personal_Loan) is also escluded.

```{r}
#copy the training and validation sets
norm_train <- train_df
norm_valid <- valid_df

#normalizing data
norm_values <- preProcess(train_df[ , 1:6], method=c("center", "scale"))

#replacing the first 6 columns with the normalized values
norm_train[ , 1:6] <- predict(norm_values, train_df[ , 1:6])
norm_valid[ , 1:6] <- predict(norm_values, valid_df[ , 1:6])
```

```{r}
#verifying min, median and max values of the normalized data
summary(norm_train)
summary(norm_valid)
```

# Tranining

Data is now ready to be using in the creation of the k-NN model.
We create the training and validation predictors, using all the variables excluding Personal_Loan, and the training and validation labels, using the variable we want the model to predict. 

```{r}
#predictors excluding Personal_Loan 
train_predictors <- norm_train[ , -7]
valid_predictors <- norm_valid[ , -7]
#label from Personal_Loan
train_labels <- norm_train[ , 7]
valid_labels <- norm_valid[ , 7]

#k-NN Model with k=1
set.seed(1234)
predicted_labels <- knn(train_predictors, valid_predictors, cl=train_labels, k=1)
```

```{r}
#viewing the result of the model
head(predicted_labels)
summary(predicted_labels)
```

# Confusion Matrix

We can now create a Confusion Matrix and then check the labels from the validation set and the predicted labels for Personal_Loan.

```{r}
c_matrix <- CrossTable(x=valid_labels, y=predicted_labels, prop.chisq=FALSE)

accuracy <- (c_matrix$t[2,2] + c_matrix$t[1,1])/ sum(c_matrix$t)
print(accuracy)
recall <- c_matrix$t[2,2]/ (c_matrix$t[2,2] + c_matrix$t[2,1])
print(recall)
precision <- c_matrix$t[2,2]/ (c_matrix$t[2,2] + c_matrix$t[1,2])
print(precision)
specificity <- c_matrix$t[1,1]/ (c_matrix$t[1,1] + c_matrix$t[1,2])
print(specificity)
```

The model wrongly predicted 72 outcomes.
From the Matrix we see the model's performance:
- Accuracy = 96.4%
- Sensitivity = 69.1%
- Precision = 88.7%
- Specificity = 99.1%



## Testing the Model with a new observation

# Add a new customer

The model has to be tested with new data to evaluate its performance.
We add a new customer of Universal Bank with the following characteristics:

```{r}
new_cust <- data.frame(
  "Age" = as.integer(40), 
  "Experience" = as.integer(10), 
  "Income" = as.integer(84), 
  "Family" = as.integer(2), 
  "CCAvg" = as.double(2), 
  "Mortgage" = as.integer(0), 
  "Personal.Loan" = NA,
  "Securities.Account" = as.factor(0), 
  "CD.Account" = as.factor(0), 
  "Online" = as.factor(1), 
  "CreditCard" = as.factor(1), 
  "Education_1" = as.factor(0), 
  "Education_2" = as.factor(1), 
  "Education_3" = as.factor(0))
head(new_cust)
```

Of course, we now all of the predictors but we do not know if he would accept the offer of a personal loan. This is why we created the model.

norm_new_cust <- new_cust
norm_new_cust[ , 1:6] <- predict(norm_values, new_cust[ , 1:6])
head(norm_new_cust)




## Partitioning of data into Training, Validation and Test sets

```{r}
set.seed(1234)
sample2 <- createDataPartition(subset$Income, p=0.80, list=FALSE)
traval_df2 = subset[sample2, ]
test_df2 = subset[-sample2, ]

sample3 = createDataPartition(traval_df2$Income, p=0.50, list=FALSE)
train_df2 = subset[sample3, ]
valid_df2 = subset[-sample3, ]
```

```{r}
summary(train_df2$Income)
summary(valid_df2$Income)
summary(test_df2$Income)
```
